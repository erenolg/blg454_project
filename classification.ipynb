{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 1,
=======
   "execution_count": 25,
>>>>>>> Stashed changes
   "id": "6e80aa77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< Updated upstream
      "Requirement already satisfied: torch in c:\\users\\90538\\anaconda3\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\90538\\anaconda3\\lib\\site-packages (0.15.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\90538\\anaconda3\\lib\\site-packages (from torch) (2.8.4)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\90538\\anaconda3\\lib\\site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\90538\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\90538\\anaconda3\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\90538\\anaconda3\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: requests in c:\\users\\90538\\anaconda3\\lib\\site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\90538\\anaconda3\\lib\\site-packages (from torchvision) (1.23.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\90538\\anaconda3\\lib\\site-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\90538\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\90538\\anaconda3\\lib\\site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\90538\\anaconda3\\lib\\site-packages (from requests->torchvision) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\90538\\anaconda3\\lib\\site-packages (from requests->torchvision) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\90538\\anaconda3\\lib\\site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\90538\\anaconda3\\lib\\site-packages (from sympy->torch) (1.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
=======
      "Requirement already satisfied: torch in c:\\users\\hp\\anaconda3\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\hp\\anaconda3\\lib\\site-packages (0.15.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch) (3.6.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch) (1.10.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch) (2.7.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torchvision) (9.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torchvision) (2.27.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torchvision) (1.22.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->torchvision) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->torchvision) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->torchvision) (2022.9.24)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sympy->torch) (1.2.1)\n"
>>>>>>> Stashed changes
     ]
    }
   ],
   "source": [
    "pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 1,
=======
   "execution_count": 26,
>>>>>>> Stashed changes
   "id": "08474140",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
<<<<<<< Updated upstream
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcc33c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.models import googlenet\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Select a style transfer model and style dataset\n",
    "style_model = googlenet(weights=True)  # Pretrained GoogLeNet model\n",
    "\n",
    "# Step 2: Choose a dataset\n",
    "# Assuming you have downloaded and extracted the TinyImageNet dataset\n",
    "\n",
    "# Step 3: Define a custom dataset for style transfer\n",
    "class StyleTransferDataset(Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        self.data = torchvision.datasets.ImageFolder('C:\\\\Users\\\\90538\\\\Desktop\\\\data\\\\tiny-imagenet-200\\\\train_try\\\\n01443537', transform=transform)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, _ = self.data[index]\n",
    "        return img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "# Step 4: Define contrastive learning method\n",
    "def contrastive_loss(image1, image2, temperature=0.5):\n",
    "    # Enable requires_grad flag for the tensors\n",
    "    image1.requires_grad_()\n",
    "    image2.requires_grad_()\n",
    "\n",
    "    # Normalize the image tensors\n",
    "    image1 = F.normalize(image1, dim=1)\n",
    "    image2 = F.normalize(image2, dim=1)\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    similarity = F.cosine_similarity(image1, image2, dim=1) / temperature\n",
    "\n",
    "    # Calculate contrastive loss\n",
    "    loss = -torch.log(torch.exp(similarity).sum() / torch.exp(similarity).diag().sum())\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Step 5: Define a linear classifier on top of the trained model\n",
    "class LinearClassifier(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Step 6: Prepare the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet normalization\n",
    "])\n",
    "\n",
    "dataset = StyleTransferDataset(transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Step 7: Train the style transfer model with contrastive learning\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "style_model.to(device)\n",
    "style_model.eval()\n",
    "\n",
    "contrastive_optimizer = optim.Adam(style_model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10):\n",
    "    for images in dataloader:\n",
    "        images = images.to(device)\n",
    "\n",
    "        # Generate positive and negative samples\n",
    "        with torch.no_grad():\n",
    "            features = style_model(images)\n",
    "        features = F.normalize(features, dim=1)\n",
    "\n",
    "        positive_samples = features\n",
    "        negative_samples = features[torch.randperm(features.size(0))]\n",
    "\n",
    "        # Calculate contrastive loss\n",
    "        contrastive_loss_value = contrastive_loss(positive_samples, negative_samples)\n",
    "\n",
    "        # Optimize the contrastive model\n",
    "        contrastive_optimizer.zero_grad()\n",
    "        contrastive_loss_value.backward()\n",
    "        contrastive_optimizer.step()\n",
    "\n",
    "# Step 8: Freeze the trained model and train a linear classifier on top\n",
    "contrastive_model = style_model\n",
    "contrastive_model.eval()\n",
    "\n",
    "linear_classifier = LinearClassifier(1024, num_classes=200)  # TinyImageNet has 200 classes\n",
    "linear_classifier.to(device)\n",
    "\n",
    "linear_classifier_optimizer = optim.Adam(linear_classifier.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10):\n",
    "    for images in dataloader:\n",
    "        images = images.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            features = contrastive_model(images)\n",
    "        features = F.normalize(features, dim=1)\n",
    "\n",
    "        # Train the linear classifier\n",
    "        linear_classifier_optimizer.zero_grad()\n",
    "        outputs = linear_classifier(features)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        loss.backward()\n",
    "        linear_classifier_optimizer.step()\n",
    "\n",
    "# Step 9: Calculate the performance of the model\n",
    "# You can use a separate validation dataset or perform cross-validation to evaluate the performance of the model.\n",
    "\n",
    "# Step 10: Compare the results with the literature\n"
=======
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a716a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import urllib.request\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# get the \"features\" portion of VGG19 (we will not need the \"classifier\" portion)\n",
    "vgg = models.vgg19(pretrained=True).features\n",
    "\n",
    "# freeze all VGG parameters since we're only optimizing the target image\n",
    "for param in vgg.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "# move the model to GPU, if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "vgg.to(device)\n",
    "\n",
    "def load_image(img_url, max_size=400, shape=None):\n",
    "    response = requests.get(img_url)\n",
    "    image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "\n",
    "    if max(image.size) > max_size:\n",
    "        size = max_size\n",
    "    else:\n",
    "        size = max(image.size)\n",
    "\n",
    "    if shape is not None:\n",
    "        size = shape\n",
    "\n",
    "    in_transform = transforms.Compose([\n",
    "                        transforms.Resize(size),\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                                             (0.229, 0.224, 0.225))])\n",
    "\n",
    "    image = in_transform(image)[:3,:,:].unsqueeze(0)\n",
    "    return image\n",
    "\n",
    "def get_features(image, model, layers=None):\n",
    "    if layers is None:\n",
    "        layers = {'0': 'conv1_1',\n",
    "                  '5': 'conv2_1', \n",
    "                  '10': 'conv3_1', \n",
    "                  '19': 'conv4_1',\n",
    "                  '21': 'conv4_2',  ## content extraction\n",
    "                  '28': 'conv5_1'}\n",
    "        \n",
    "    features = {}\n",
    "    x = image\n",
    "    for name, layer in model._modules.items():\n",
    "        x = layer(x)\n",
    "        if name in layers:\n",
    "            features[layers[name]] = x\n",
    "            \n",
    "    return features\n",
    "\n",
    "# Function to convert a tensor to an image\n",
    "def im_convert(tensor):\n",
    "    image = tensor.to(\"cpu\").clone().detach()\n",
    "    image = image.numpy().squeeze()\n",
    "    image = image.transpose(1,2,0)\n",
    "    image = image * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))\n",
    "    image = image.clip(0, 1)\n",
    "    return image\n",
    "\n",
    "def gram_matrix(tensor):\n",
    "    _, d, h, w = tensor.size()\n",
    "    tensor = tensor.view(d, h * w)\n",
    "    gram = torch.mm(tensor, tensor.t())\n",
    "    return gram\n",
    "\n",
    "# load in content and style image\n",
    "content = load_image('https://upload.wikimedia.org/wikipedia/commons/thumb/0/00/Tuebingen_Neckarfront.jpg/640px-Tuebingen_Neckarfront.jpg').to(device)\n",
    "style = load_image('https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg/450px-Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg', shape=content.shape[-2:]).to(device)\n",
    "\n",
    "# create a third \"target\" image and prep it for change\n",
    "# it's a good idea to start of with the target as a copy of our *content* image\n",
    "# then iteratively change its style\n",
    "target = content.clone().requires_grad_(True).to(device)\n",
    "\n",
    "# compute and store the Gram Matrices for the style image\n",
    "style_features = get_features(style, vgg)\n",
    "style_grams = {layer: gram_matrix(style_features[layer]) for layer in style_features}\n",
    "\n",
    "# weights for each style layer \n",
    "# weighting earlier layers more will result in *larger* style artifacts\n",
    "# notice we are excluding `conv4_2` our content representation\n",
    "style_weights = {'conv1_1': 1.,\n",
    "                 'conv2_1': 0.8,\n",
    "                 'conv3_1': 0.5,\n",
    "                 'conv4_1': 0.3,\n",
    "                 'conv5_1': 0.1}\n",
    "\n",
    "content_weight = 1  # alpha\n",
    "style_weight = 1e6  # beta\n",
    "\n",
    "# iteration hyperparameters\n",
    "optimizer = optim.Adam([target], lr=0.003)\n",
    "steps = 2000  # decide how many iterations to update your image (5000)\n",
    "\n",
    "show_every = 400\n",
    "\n",
    "# for displaying the target image, intermittently\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(im_convert(target))\n",
    "\n",
    "# iteration loop\n",
    "for ii in range(1, steps+1):\n",
    "\n",
    "    target_features = get_features(target, vgg)\n",
    "    content_features = get_features(content, vgg)\n",
    "    style_features = get_features(style, vgg)\n",
    "\n",
    "    content_loss = torch.mean((target_features['conv4_2'] - content_features['conv4_2'])**2)\n",
    "\n",
    "    # the style loss\n",
    "    # initialize the style loss to 0\n",
    "    style_loss = 0\n",
    "    # then add to it for each layer's gram matrix loss\n",
    "    for layer in style_weights:\n",
    "        # get the \"target\" style representation for the layer\n",
    "        target_feature = target_features[layer]\n",
    "        target_gram = gram_matrix(target_feature)\n",
    "        _, d, h, w = target_feature.shape\n",
    "        # get the \"style\" style representation\n",
    "        style_gram = style_grams[layer]\n",
    "        # the style loss for one layer, weighted appropriately\n",
    "        layer_style_loss = style_weights[layer] * torch.mean((target_gram - style_gram)**2)\n",
    "        # add to the style loss\n",
    "        style_loss += layer_style_loss / (d * h * w)\n",
    "\n",
    "    # calculate the total loss\n",
    "    total_loss = content_weight * content_loss + style_weight * style_loss\n",
    "\n",
    "    # update your target image\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # display intermediate images and print the loss\n",
    "    if  ii % show_every == 0:\n",
    "        print('Total loss: ', total_loss.item())\n",
    "        plt.imshow(im_convert(target))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcc33c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torchvision\n",
    "# import torchvision.transforms as transforms\n",
    "# from torch.utils.data import DataLoader, Dataset\n",
    "# from torchvision.models import googlenet\n",
    "# import torch.optim as optim\n",
    "# import torch.nn.functional as F\n",
    "# import numpy as np\n",
    "\n",
    "# # Step 1: Select a style transfer model and style dataset\n",
    "# style_model = googlenet(pretrained=True)  # Pretrained GoogLeNet model\n",
    "\n",
    "# # Step 2: Choose a dataset\n",
    "# # Assuming you have downloaded and extracted the TinyImageNet dataset\n",
    "\n",
    "# # Step 3: Define a custom dataset for style transfer\n",
    "# class StyleTransferDataset(Dataset):\n",
    "#     def __init__(self, transform=None):\n",
    "#         self.data = torchvision.datasets.ImageFolder('path_to_tiny_imagenet/train', transform=transform)\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         img, _ = self.data[index]\n",
    "#         return img\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "# # Step 4: Define contrastive learning method\n",
    "# def contrastive_loss(image1, image2, temperature=0.5):\n",
    "#     # Normalize the image tensors\n",
    "#     image1 = F.normalize(image1, dim=1)\n",
    "#     image2 = F.normalize(image2, dim=1)\n",
    "\n",
    "#     # Calculate cosine similarity\n",
    "#     similarity = F.cosine_similarity(image1, image2, dim=1) / temperature\n",
    "\n",
    "#     # Calculate contrastive loss\n",
    "#     loss = -torch.log(torch.exp(similarity).sum() / torch.exp(similarity).diag().sum())\n",
    "\n",
    "#     return loss\n",
    "\n",
    "# # Step 5: Define a linear classifier on top of the trained model\n",
    "# class LinearClassifier(nn.Module):\n",
    "#     def __init__(self, input_size, num_classes):\n",
    "#         super(LinearClassifier, self).__init__()\n",
    "#         self.fc = nn.Linear(input_size, num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.fc(x)\n",
    "#         return x\n",
    "\n",
    "# # Step 6: Prepare the data\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet normalization\n",
    "# ])\n",
    "\n",
    "# dataset = StyleTransferDataset(transform=transform)\n",
    "# dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# # Step 7: Train the style transfer model with contrastive learning\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# style_model.to(device)\n",
    "# style_model.eval()\n",
    "\n",
    "# contrastive_optimizer = optim.Adam(style_model.parameters(), lr=0.001)\n",
    "\n",
    "# for epoch in range(10):\n",
    "#     for images in dataloader:\n",
    "#         images = images.to(device)\n",
    "\n",
    "#         # Generate positive and negative samples\n",
    "#         with torch.no_grad():\n",
    "#             features = style_model(images)\n",
    "#         features = F.normalize(features, dim=1)\n",
    "\n",
    "#         positive_samples = features\n",
    "#         negative_samples = features[torch.randperm(features.size(0))]\n",
    "\n",
    "#         # Calculate contrastive loss\n",
    "#         contrastive_loss_value = contrastive_loss(positive_samples, negative_samples)\n",
    "\n",
    "#         # Optimize the style model\n",
    "#         contrastive_optimizer.zero_grad()\n",
    "#         contrastive_loss_value.backward()\n",
    "#         contrastive_optimizer.step()\n",
    "\n",
    "# # Step 8: Freeze the trained model and train a linear classifier on top\n",
    "# style_model.eval()\n",
    "# linear_classifier = LinearClassifier(1024, num_classes=200)  # TinyImageNet has 200 classes\n",
    "# linear_classifier.to(device)\n",
    "\n",
    "# linear_classifier_optimizer = optim.Adam(linear_classifier.parameters(), lr=0.001)\n",
    "\n",
    "# for epoch in range(10):\n",
    "#     for images in dataloader:\n",
    "#         images = images.to(device)\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             features = style_model(images)\n",
    "#         features = F.normalize(features, dim=1)\n",
    "\n",
    "#         # Train the linear classifier\n",
    "#         linear_classifier_optimizer.zero_grad()\n",
    "#         outputs = linear_classifier(features)\n",
    "#         loss = F.cross_entropy(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         linear_classifier_optimizer.step()\n",
    "\n",
    "# # Step 9: Calculate the performance of the model\n",
    "# # You can use a separate validation dataset or perform cross-validation to evaluate the performance of the model.\n",
    "\n",
    "# # Step 10: Compare the results with the literature\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
